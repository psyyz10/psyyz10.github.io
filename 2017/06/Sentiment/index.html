<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Sentiment | YAO&#39;s BLOG</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Sentiment Classification on Large Movie ReviewsSentiment Analysis is understood as a classic natural language processing problem. In this example, a large moview review dataset was chosen from IMDB to">
<meta property="og:type" content="article">
<meta property="og:title" content="Sentiment">
<meta property="og:url" content="http://psyyz10.github.io/2017/06/Sentiment/index.html">
<meta property="og:site_name" content="YAO's BLOG">
<meta property="og:description" content="Sentiment Classification on Large Movie ReviewsSentiment Analysis is understood as a classic natural language processing problem. In this example, a large moview review dataset was chosen from IMDB to">
<meta property="og:image" content="http://psyyz10.github.io/output_3_1.png">
<meta property="og:image" content="http://psyyz10.github.io/output_26_1.png">
<meta property="og:updated_time" content="2017-06-02T09:41:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sentiment">
<meta name="twitter:description" content="Sentiment Classification on Large Movie ReviewsSentiment Analysis is understood as a classic natural language processing problem. In this example, a large moview review dataset was chosen from IMDB to">
  
    <link rel="alternative" href="/atom.xml" title="YAO&#39;s BLOG" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YAO&#39;s BLOG</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">A personal blog of Yao Zhang</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" href="/projects">Project</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://psyyz10.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Sentiment" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/06/Sentiment/" class="article-date">
  <time datetime="2017-06-02T09:39:44.000Z" itemprop="datePublished">2017-06-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Sentiment
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Sentiment_Classification_on_Large_Movie_Reviews">Sentiment Classification on Large Movie Reviews</h2><p><a href="https://en.wikipedia.org/wiki/Sentiment_analysis" target="_blank" rel="external">Sentiment Analysis</a> is understood as a classic natural language processing problem. In this example, a large moview review dataset was chosen from IMDB to do a sentiment classification task with some deep learning approaches. The labeled data set consists of 50,000 <a href="http://www.imdb.com/" target="_blank" rel="external">IMDB</a> movie reviews (good or bad), in which 25000 highly polar movie reviews for training, and 25,000 for testing. The dataset is originally collected by Stanford researchers and was used in a <a href="http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf" target="_blank" rel="external">2011 paper</a>, and the highest accuray of 88.33% was achieved without using the unbalanced data. This example illustrates some deep learning approaches to do the sentiment classification with <a href="https://github.com/intel-analytics/BigDL" target="_blank" rel="external">BigDL</a> python API.</p>
<h3 id="Load_the_IMDB_Dataset">Load the IMDB Dataset</h3><p>The IMDB dataset need to be loaded into BigDL, note that the dataset has been pre-processed, and each review was encoded as a sequence of integers. Each integer represents the index of the overall frequency of dataset, for instance, ‘5’ means the 5-th most frequent words occured in the data. It is very convinient to filter the words by some conditions, for example, to filter only the top 5,000 most common word and/or eliminate the top 30 most common words. Let’s define functions to load the pre-processed data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> base</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_imdb</span><span class="params">(dest_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Download pre-processed IMDB movie review data</span><br><span class="line"></span><br><span class="line">    :argument</span><br><span class="line">        dest_dir: destination directory to store the data</span><br><span class="line"></span><br><span class="line">    :return</span><br><span class="line">        The absolute path of the stored data</span><br><span class="line">    """</span></span><br><span class="line">    file_name = <span class="string">"imdb.npz"</span></span><br><span class="line">    file_abs_path = base.maybe_download(file_name,</span><br><span class="line">                                        dest_dir,</span><br><span class="line">                                        <span class="string">'https://s3.amazonaws.com/text-datasets/imdb.npz'</span>)</span><br><span class="line">    <span class="keyword">return</span> file_abs_path</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_imdb</span><span class="params">(dest_dir=<span class="string">'/tmp/.bigdl/dataset'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Load IMDB dataset.</span><br><span class="line"></span><br><span class="line">    :argument</span><br><span class="line">        dest_dir: where to cache the data (relative to `~/.bigdl/dataset`).</span><br><span class="line"></span><br><span class="line">    :return</span><br><span class="line">        the train, test separated IMDB dataset.</span><br><span class="line">    """</span></span><br><span class="line">    path = download_imdb(dest_dir)</span><br><span class="line">    f = np.load(path)</span><br><span class="line">    x_train = f[<span class="string">'x_train'</span>]</span><br><span class="line">    y_train = f[<span class="string">'y_train'</span>]</span><br><span class="line">    x_test = f[<span class="string">'x_test'</span>]</span><br><span class="line">    y_test = f[<span class="string">'y_test'</span>]</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (x_train, y_train), (x_test, y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Processing text dataset'</span>)</span><br><span class="line">(x_train, y_train), (x_test, y_test) = load_imdb()</span><br><span class="line">print(<span class="string">'finished processing text'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Processing <span class="type">text</span> dataset
finished processing <span class="type">text</span>
</code></pre><p>In order to set a proper max sequence length, we need to go througth the property of the data and see the length distribution of each sentence in the dataset. A box and whisker plot is shown below for reviewing the length distribution in words.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Summarize review length</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Review length: "</span>)</span><br><span class="line">X = np.concatenate((x_train, x_test), axis=<span class="number">0</span>)</span><br><span class="line">result = [len(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">print(<span class="string">"Mean %.2f words (%f)"</span> % (np.mean(result), np.std(result)))</span><br><span class="line"><span class="comment"># plot review length</span></span><br><span class="line"><span class="comment"># Create a figure instance</span></span><br><span class="line">fig = pyplot.figure(<span class="number">1</span>, figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line">pyplot.boxplot(result)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure>
<pre><code>Review <span class="property">length</span>: 
Mean <span class="number">233.76</span> <span class="property">words</span> (<span class="number">172.911495</span>)
</code></pre><p><img src="output_3_1.png" alt="png"></p>
<p>Looking the box and whisker plot, the max length of a sample in words is 500, and the mean and median are below 250. According to the plot, we can probably cover the mass of the distribution with a clipped length of 400 to 500. Here we set the max sequence length of each sample as 500.</p>
<p>The corresponding vocabulary sorted by frequency is also required, for further embedding the words with pre-trained vectors. The downloaded vocabulary is in {word: index}, where each word as a key and the index as a value. It needs to be transformed into {index: word} format.</p>
<p>Let’s define a function to obtain the vocabulary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_word_index</span><span class="params">(dest_dir=<span class="string">'/tmp/.bigdl/dataset'</span>, )</span>:</span></span><br><span class="line">    <span class="string">"""Retrieves the dictionary mapping word indices back to words.</span><br><span class="line"></span><br><span class="line">    :argument</span><br><span class="line">        path: where to cache the data (relative to `~/.bigdl/dataset`).</span><br><span class="line"></span><br><span class="line">    :return</span><br><span class="line">        The word index dictionary.</span><br><span class="line">    """</span></span><br><span class="line">    file_name = <span class="string">"imdb_word_index.json"</span></span><br><span class="line">    path = base.maybe_download(file_name,</span><br><span class="line">                               dest_dir,</span><br><span class="line">                               source_url=<span class="string">'https://s3.amazonaws.com/text-datasets/imdb_word_index.json'</span>)</span><br><span class="line">    f = open(path)</span><br><span class="line">    data = json.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Processing vocabulary'</span>)</span><br><span class="line">word_idx = get_word_index()</span><br><span class="line">idx_word = &#123;v:k <span class="keyword">for</span> k,v <span class="keyword">in</span> word_idx.items()&#125;</span><br><span class="line">print(<span class="string">'finished processing vocabulary'</span>)</span><br></pre></td></tr></table></figure>
<pre><code><span class="title">Processing</span> vocabulary
finished processing vocabulary
</code></pre><h3 id="Text_pre-processing">Text pre-processing</h3><p>Before we train the network, some pre-processing steps need to be applied to the dataset. </p>
<p>Next let’s go through the mechanisms that used to be applied to the data.</p>
<ul>
<li><p>We insert a <code>start_char</code> at the beginning of each sentence to mark the start point. We set it as <code>2</code> here, and each other word index will plus a constant <code>index_from</code> to differentiate some ‘helper index’ (eg. <code>start_char</code>, <code>oov_char</code>, etc.).</p>
</li>
<li><p>A <code>max_words</code> variable is defined as the maximum index number (the least frequent word) included in the sequence. If the word index number is larger than <code>max_words</code>, it will be replaced by a out-of-vocabulary number <code>oov_char</code>, which is <code>3</code> here.</p>
</li>
<li><p>Each word index sequence is restricted to the same length. We used left-padding here, which means the right (end) of the sequence will be keep as many as possible and drop the left (head) of the sequence if its length is more than pre-defined <code>sequence_len</code>, or padding the left (head) of the sequence with <code>padding_value</code>.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_oov</span><span class="params">(x, oov_char, max_words)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Replace the words out of vocabulary with `oov_char`</span><br><span class="line">    :param x: a sequence</span><br><span class="line">    :param max_words: the max number of words to include</span><br><span class="line">    :param oov_char: words out of vocabulary because of exceeding the `max_words`</span><br><span class="line">        limit will be replaced by this character</span><br><span class="line"></span><br><span class="line">    :return: The replaced sequence</span><br><span class="line">    """</span></span><br><span class="line">    <span class="keyword">return</span> [oov_char <span class="keyword">if</span> w &gt;= max_words <span class="keyword">else</span> w <span class="keyword">for</span> w <span class="keyword">in</span> x]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sequence</span><span class="params">(x, fill_value, length)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Pads each sequence to the same length</span><br><span class="line">    :param x: a sequence</span><br><span class="line">    :param fill_value: pad the sequence with this value</span><br><span class="line">    :param length: pad sequence to the length</span><br><span class="line"></span><br><span class="line">    :return: the padded sequence</span><br><span class="line">    """</span></span><br><span class="line">    <span class="keyword">if</span> len(x) &gt;= length:</span><br><span class="line">        <span class="keyword">return</span> x[(len(x) - length):]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> [fill_value] * (length - len(x)) + x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_sample</span><span class="params">(features, label)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Wrap the `features` and `label` to a training sample object</span><br><span class="line">    :param features: features of a sample</span><br><span class="line">    :param label: label of a sample</span><br><span class="line">    </span><br><span class="line">    :return: a sample object including features and label</span><br><span class="line">    """</span></span><br><span class="line">    <span class="keyword">return</span> Sample.from_ndarray(np.array(features, dtype=<span class="string">'float'</span>), np.array(label))</span><br><span class="line"></span><br><span class="line">padding_value = <span class="number">1</span></span><br><span class="line">start_char = <span class="number">2</span></span><br><span class="line">oov_char = <span class="number">3</span></span><br><span class="line">index_from = <span class="number">3</span></span><br><span class="line">max_words = <span class="number">5000</span></span><br><span class="line">sequence_len = <span class="number">500</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'start transformation'</span>)</span><br><span class="line"></span><br><span class="line">train_rdd = sc.parallelize(zip(x_train, y_train), <span class="number">2</span>) \</span><br><span class="line">        .map(<span class="keyword">lambda</span> (x, y): ([start_char] + [w + index_from <span class="keyword">for</span> w <span class="keyword">in</span> x] , y))\</span><br><span class="line">        .map(<span class="keyword">lambda</span> (x, y): (replace_oov(x, oov_char, max_words), y))\</span><br><span class="line">        .map(<span class="keyword">lambda</span> (x, y): (pad_sequence(x, padding_value, sequence_len), y))\</span><br><span class="line">        .map(<span class="keyword">lambda</span> (x, y): to_sample(x, y))</span><br><span class="line">test_rdd = sc.parallelize(zip(x_test, y_test), <span class="number">2</span>) \</span><br><span class="line">        .map(<span class="keyword">lambda</span> (x, y): ([start_char] + [w + index_from <span class="keyword">for</span> w <span class="keyword">in</span> x], y))\</span><br><span class="line">        .map(<span class="keyword">lambda</span> (x, y): (replace_oov(x, oov_char, max_words), y))\</span><br><span class="line">        .map(<span class="keyword">lambda</span> (x, y): (pad_sequence(x, padding_value, sequence_len), y))\</span><br><span class="line">        .map(<span class="keyword">lambda</span> (x, y): to_sample(x, y))</span><br><span class="line">        </span><br><span class="line">print(<span class="string">'finish transformation'</span>)</span><br></pre></td></tr></table></figure>
<pre><code><span class="operator"><span class="keyword">start</span> transformation
<span class="keyword">finish</span> transformation</span>
</code></pre><h3 id="Word_Embedding">Word Embedding</h3><p><a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank" rel="external">Word embedding</a> is a recent breakthrough in natural language field. The key idea is to encode words and phrases into distributed representations in the format of word vectors, which means each word is represented as a vector. There are two widely used word vector training alogirhms, one is published by Google called <a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="external">word to vector</a>, the other is published by Standford called <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="external">Glove</a>. In this example, pre-trained glove is loaded into a lookup table and will be fine-tuned during the training process. BigDL provides a method to download and load glove in <code>news20</code> package.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> news20</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'loading glove'</span>)</span><br><span class="line">glove = news20.get_glove_w2v(source_dir=<span class="string">'/tmp/.bigdl/dataset'</span>, dim=embedding_dim)</span><br><span class="line">print(<span class="string">'finish loading glove'</span>)</span><br></pre></td></tr></table></figure>
<pre><code><span class="title">loading</span> glove
finish loading glove
</code></pre><p>For each word whose index less than the <code>max_word</code> should try to match its embedding and store in an array.</p>
<p>With regard to those words which can not be found in glove, we randomly sample it from a [-0.05, 0.05] uniform distribution.</p>
<p>BigDL usually use a <code>LookupTable</code> layer to do word embedding, so the matrix will be loaded to the LookupTable by seting the weight.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'processing glove'</span>)</span><br><span class="line">w2v = [glove.get(idx_word.get(i - index_from), np.random.uniform(-<span class="number">0.05</span>, <span class="number">0.05</span>, embedding_dim))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, max_words + <span class="number">1</span>)]</span><br><span class="line">w2v = np.array(list(itertools.chain(*np.array(w2v, dtype=<span class="string">'float'</span>))), dtype=<span class="string">'float'</span>) \</span><br><span class="line">        .reshape([max_words, embedding_dim])</span><br><span class="line">print(<span class="string">'finish processing glove'</span>)</span><br></pre></td></tr></table></figure>
<pre><code><span class="title">processing</span> glove
finish processing glove
</code></pre><h3 id="Build_models">Build models</h3><p>Next, let’s build some deep learning models for the sentiment classification. </p>
<p>As an example, several deep learning models are illustrated for tutorial, comparison and demonstration.</p>
<p><strong>LSTM</strong>, <strong>GRU</strong>, <strong>Bi-LSTM</strong>, <strong>CNN</strong> and <strong>CNN + LSTM</strong> models are implemented as options. To decide which model to use, just assign model_type the corresponding string.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nn.layer <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> util.common <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">p = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(w2v)</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line"></span><br><span class="line">    embedding = LookupTable(max_words, embedding_dim)</span><br><span class="line">    embedding.set_weights([w2v])</span><br><span class="line">    model.add(embedding)</span><br><span class="line">    <span class="keyword">if</span> model_type.lower() == <span class="string">"gru"</span>:</span><br><span class="line">        model.add(Recurrent()</span><br><span class="line">                .add(GRU(embedding_dim, <span class="number">128</span>, p))) \</span><br><span class="line">            .add(Select(<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">elif</span> model_type.lower() == <span class="string">"lstm"</span>:</span><br><span class="line">        model.add(Recurrent()</span><br><span class="line">                  .add(LSTM(embedding_dim, <span class="number">128</span>, p)))\</span><br><span class="line">            .add(Select(<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">elif</span> model_type.lower() == <span class="string">"bi_lstm"</span>:</span><br><span class="line">        model.add(BiRecurrent(CAddTable())</span><br><span class="line">                  .add(LSTM(embedding_dim, <span class="number">128</span>, p)))\</span><br><span class="line">            .add(Select(<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">elif</span> model_type.lower() == <span class="string">"cnn"</span>:</span><br><span class="line">        model.add(Transpose([(<span class="number">2</span>, <span class="number">3</span>)]))\</span><br><span class="line">            .add(Dropout(p))\</span><br><span class="line">            .add(Reshape([embedding_dim, <span class="number">1</span>, sequence_len]))\</span><br><span class="line">            .add(SpatialConvolution(embedding_dim, <span class="number">128</span>, <span class="number">5</span>, <span class="number">1</span>))\</span><br><span class="line">            .add(ReLU())\</span><br><span class="line">            .add(SpatialMaxPooling(sequence_len - <span class="number">5</span> + <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))\</span><br><span class="line">            .add(Reshape([<span class="number">128</span>]))</span><br><span class="line">    <span class="keyword">elif</span> model_type.lower() == <span class="string">"cnn_lstm"</span>:</span><br><span class="line">        model.add(Transpose([(<span class="number">2</span>, <span class="number">3</span>)]))\</span><br><span class="line">            .add(Dropout(p))\</span><br><span class="line">            .add(Reshape([embedding_dim, <span class="number">1</span>, sequence_len])) \</span><br><span class="line">            .add(SpatialConvolution(embedding_dim, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>)) \</span><br><span class="line">            .add(ReLU()) \</span><br><span class="line">            .add(SpatialMaxPooling(<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)) \</span><br><span class="line">            .add(Squeeze(<span class="number">3</span>)) \</span><br><span class="line">            .add(Transpose([(<span class="number">2</span>, <span class="number">3</span>)])) \</span><br><span class="line">            .add(Recurrent()</span><br><span class="line">                 .add(LSTM(<span class="number">64</span>, <span class="number">128</span>, p))) \</span><br><span class="line">            .add(Select(<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    model.add(Linear(<span class="number">128</span>, <span class="number">100</span>))\</span><br><span class="line">        .add(Dropout(<span class="number">0.2</span>))\</span><br><span class="line">        .add(ReLU())\</span><br><span class="line">        .add(Linear(<span class="number">100</span>, <span class="number">1</span>))\</span><br><span class="line">        .add(Sigmoid())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h3 id="Optimization">Optimization</h3><p><code>Optimizer</code> need to be created to optimise the model.</p>
<p>Here we use the <code>CNN</code> model.</p>
<p>More details about optimizer in BigDL, please refer to <a href="https://github.com/intel-analytics/BigDL/wiki/Programming-Guide#optimizer" target="_blank" rel="external">Programming Guide</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> optim.optimizer <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> nn.criterion <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">max_epoch = <span class="number">4</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">model_type = <span class="string">'gru'</span></span><br><span class="line"></span><br><span class="line">init_engine()</span><br><span class="line"></span><br><span class="line">optimizer = Optimizer(</span><br><span class="line">        model=build_model(w2v),</span><br><span class="line">        training_rdd=train_rdd,</span><br><span class="line">        criterion=BCECriterion(),</span><br><span class="line">        end_trigger=MaxEpoch(max_epoch),</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        optim_method=Adam())</span><br><span class="line"></span><br><span class="line">optimizer.set_validation(</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        val_rdd=test_rdd,</span><br><span class="line">        trigger=EveryEpoch(),</span><br><span class="line">        val_method=[<span class="string">"Top1Accuracy"</span>])</span><br></pre></td></tr></table></figure>
<pre><code><span class="attribute">creating</span>: <span class="string">createSequential</span>
<span class="attribute">creating</span>: <span class="string">createLookupTable</span>
<span class="attribute">creating</span>: <span class="string">createRecurrent</span>
<span class="attribute">creating</span>: <span class="string">createGRU</span>
<span class="attribute">creating</span>: <span class="string">createSelect</span>
<span class="attribute">creating</span>: <span class="string">createLinear</span>
<span class="attribute">creating</span>: <span class="string">createDropout</span>
<span class="attribute">creating</span>: <span class="string">createReLU</span>
<span class="attribute">creating</span>: <span class="string">createLinear</span>
<span class="attribute">creating</span>: <span class="string">createSigmoid</span>
<span class="attribute">creating</span>: <span class="string">createBCECriterion</span>
<span class="attribute">creating</span>: <span class="string">createMaxEpoch</span>
<span class="attribute">creating</span>: <span class="string">createAdam</span>
<span class="attribute">creating</span>: <span class="string">createOptimizer</span>
<span class="attribute">creating</span>: <span class="string">createEveryEpoch</span>
</code></pre><p>To make the training process be visualized by TensorBoard, training summaries should be saved as a format of logs.</p>
<p>With regard to the usage of TensorBoard in BigDL, please refer to <a href="https://github.com/intel-analytics/BigDL/wiki/Visualization-with-TensorBoard" target="_blank" rel="external">BigDL Wiki</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"></span><br><span class="line">logdir = <span class="string">'/tmp/.bigdl/'</span></span><br><span class="line">app_name = <span class="string">'adam-'</span> + dt.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line"></span><br><span class="line">train_summary = TrainSummary(log_dir=logdir, app_name=app_name)</span><br><span class="line">train_summary.set_summary_trigger(<span class="string">"Parameters"</span>, SeveralIteration(<span class="number">50</span>))</span><br><span class="line">val_summary = ValidationSummary(log_dir=logdir, app_name=app_name)</span><br><span class="line">optimizer.set_train_summary(train_summary)</span><br><span class="line">optimizer.set_val_summary(val_summary)</span><br></pre></td></tr></table></figure>
<pre><code><span class="attribute">creating</span>: <span class="string">createTrainSummary</span>
<span class="attribute">creating</span>: <span class="string">createSeveralIteration</span>
<span class="attribute">creating</span>: <span class="string">createValidationSummary</span>

<span class="http">

<span class="http">

<span class="stylus">&lt;optim<span class="class">.optimizer</span><span class="class">.Optimizer</span> at <span class="number">0</span>x7fa651314ad0&gt;</span></span></span>
</code></pre><p>Now, let’s start training!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">train_model = optimizer.optimize()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Optimization Done."</span></span><br></pre></td></tr></table></figure>
<pre><code>Optimization Done.
CPU times: user <span class="number">196</span> ms, sys: <span class="number">24</span> ms, total: <span class="number">220</span> ms
Wall time: <span class="number">51</span>min <span class="number">52</span>s
</code></pre><h3 id="Test">Test</h3><p>Validation accuracy is shown in the training log, here let’s get the accuracy on validation set by hand.</p>
<p>Predict the <code>test_rdd</code> (validation set data), and obtain the predicted label and ground truth label in the list.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">predictions = train_model.predict(test_rdd)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map_predict_label</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> l &gt; <span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map_groundtruth_label</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> l[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">y_pred = np.array([ map_predict_label(s) <span class="keyword">for</span> s <span class="keyword">in</span> predictions.collect()])</span><br><span class="line"></span><br><span class="line">y_true = np.array([map_groundtruth_label(s.label) <span class="keyword">for</span> s <span class="keyword">in</span> test_rdd.collect()])</span><br></pre></td></tr></table></figure>
<p>Then let’s see the prediction accuracy on validation set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">0</span>, y_pred.size):</span><br><span class="line">    <span class="keyword">if</span> (y_pred[i] == y_true[i]):</span><br><span class="line">        correct += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">accuracy = float(correct) / y_pred.size</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Prediction accuracy on validation set is: '</span>, accuracy</span><br></pre></td></tr></table></figure>
<pre><code>Prediction accuracy <span class="function_start"><span class="keyword">on</span></span> validation <span class="keyword">set</span> <span class="keyword">is</span>:  <span class="number">0.89312</span>
</code></pre><p>Show the confusion matrix</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sn</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_true, y_pred)</span><br><span class="line">cm.shape</span><br><span class="line"></span><br><span class="line">df_cm = pd.DataFrame(cm)</span><br><span class="line">plt.figure(figsize = (<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">sn.heatmap(df_cm, annot=<span class="keyword">True</span>,fmt=<span class="string">'d'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib<span class="class">.axes</span>._subplots<span class="class">.AxesSubplot</span> at <span class="number">0</span>x7fa643fe0c50&gt;
</code></pre><p><img src="output_26_1.png" alt="png"></p>
<p>Because of the limitation of ariticle length, not all the results of optional models can be shown respectively. Please try other provided optional models to see the results. If you are interested in optimizing the results, try different training parameters which may make inpacts on the result, such as the max sequence length, batch size, training epochs, preprocessing schemes, optimization methods and so on. Among the models, CNN training would be much quicker. Note that the LSTM and it variants (eg. GRU) are difficult to train, even a unsuitable batch size may cause the model not converge. In addition it is prone to overfitting, please try different dropout threshold and/or add regularizers (abouth how to add regularizers in BigDL please see <a href="https://github.com/intel-analytics/BigDL/wiki/Programming-Guide#regularizers" target="_blank" rel="external">BigDL Wiki</a>).</p>
<h3 id="Summary">Summary</h3><p>In this example, you learned how to use BigDL to develop deep learning models for sentiment analysis including:</p>
<ul>
<li>How to load and review the IMDB dataset</li>
<li>How to do word embedding with Glove</li>
<li>How to build a CNN model for NLP with BigDL</li>
<li>How to build a LSTM model for NLP with BigDL</li>
<li>How to build a GRU model for NLP with BigDL</li>
<li>How to build a Bi-LSTM model for NLP with BigDL</li>
<li>How to build a CNN-LSTM model for NLP with BigDL</li>
<li>How to train deep learning models with BigDL</li>
</ul>
<p>Thanks for your reading, please enjoy the trip on using BigDL to build deep learning models.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://psyyz10.github.io/2017/06/Sentiment/" data-id="cj3fnu1lg001ca6u5xlji34kc" class="article-share-link">Share</a>
      
        <a href="http://psyyz10.github.io/2017/06/Sentiment/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a><span class="article-tag-list-count">4</span></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="article-tag-list-count">4</span></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/11/Deep-Belief-Network/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Deep Belief Network</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Haskell/">Haskell</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Learning-Note/">Learning Note</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leetcode-and-Lintcode/">Leetcode and Lintcode</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Array/">Array</a><span class="tag-list-count">24</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Binary-Search/">Binary Search</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bit-Manipulation/">Bit Manipulation</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/">Configuration</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DP/">DP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Enumeration/">Enumeration</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Functional-Programming/">Functional Programming</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hash-Map/">Hash Map</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Haskell/">Haskell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Interview/">Interview</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java-8/">Java 8</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linked-List/">Linked List</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markdown/">Markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Open-Courses/">Open Courses</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Priority-Queue/">Priority Queue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Programming-Language/">Programming Language</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/String/">String</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Two-Pointers/">Two Pointers</a><span class="tag-list-count">6</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Array/" style="font-size: 20px;">Array</a> <a href="/tags/Binary-Search/" style="font-size: 11.67px;">Binary Search</a> <a href="/tags/Bit-Manipulation/" style="font-size: 13.33px;">Bit Manipulation</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Enumeration/" style="font-size: 10px;">Enumeration</a> <a href="/tags/Functional-Programming/" style="font-size: 10px;">Functional Programming</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Hash-Map/" style="font-size: 11.67px;">Hash Map</a> <a href="/tags/Haskell/" style="font-size: 10px;">Haskell</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Interview/" style="font-size: 10px;">Interview</a> <a href="/tags/Java/" style="font-size: 11.67px;">Java</a> <a href="/tags/Java-8/" style="font-size: 11.67px;">Java 8</a> <a href="/tags/Linked-List/" style="font-size: 18.33px;">Linked List</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/tags/Math/" style="font-size: 11.67px;">Math</a> <a href="/tags/Open-Courses/" style="font-size: 10px;">Open Courses</a> <a href="/tags/Priority-Queue/" style="font-size: 10px;">Priority Queue</a> <a href="/tags/Programming-Language/" style="font-size: 11.67px;">Programming Language</a> <a href="/tags/String/" style="font-size: 15px;">String</a> <a href="/tags/Two-Pointers/" style="font-size: 16.67px;">Two Pointers</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">42</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/06/Sentiment/">Sentiment</a>
          </li>
        
          <li>
            <a href="/2016/11/Deep-Belief-Network/">Deep Belief Network</a>
          </li>
        
          <li>
            <a href="/2016/11/Fractals/">Fractals</a>
          </li>
        
          <li>
            <a href="/2016/05/Rotate-String/">Rotate String</a>
          </li>
        
          <li>
            <a href="/2016/05/Ugly-Number-II/">Ugly Number II</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Yao Zhang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/projects" class="mobile-nav-link">Project</a>
  
</nav>
    
<script>
  var disqus_shortname = 'httppsyyz10githubio';
  
  var disqus_url = 'http://psyyz10.github.io/2017/06/Sentiment/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>