<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Stacked Denoising Autoencoders | YAO&#39;s BLOG</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="AutoencodersAn autoencoder [4] is a network whose graphical structure is shown in Figure 4.1, which has the same dimension for both input and output. It takes an unlabeled training examples in set  wh">
<meta property="og:type" content="article">
<meta property="og:title" content="Stacked Denoising Autoencoders">
<meta property="og:url" content="http://psyyz10.github.io/2015/11/SDA/index.html">
<meta property="og:site_name" content="YAO's BLOG">
<meta property="og:description" content="AutoencodersAn autoencoder [4] is a network whose graphical structure is shown in Figure 4.1, which has the same dimension for both input and output. It takes an unlabeled training examples in set  wh">
<meta property="og:image" content="http://psyyz10.github.io/img/1.png">
<meta property="og:image" content="http://psyyz10.github.io/img/2.png">
<meta property="og:image" content="http://psyyz10.github.io/img/3.png">
<meta property="og:image" content="http://psyyz10.github.io/img/4.png">
<meta property="og:image" content="http://psyyz10.github.io/img/5.png">
<meta property="og:image" content="http://psyyz10.github.io/img/6.png">
<meta property="og:image" content="http://psyyz10.github.io/img/7.png">
<meta property="og:updated_time" content="2015-11-09T20:59:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stacked Denoising Autoencoders">
<meta name="twitter:description" content="AutoencodersAn autoencoder [4] is a network whose graphical structure is shown in Figure 4.1, which has the same dimension for both input and output. It takes an unlabeled training examples in set  wh">
  
    <link rel="alternative" href="/atom.xml" title="YAO&#39;s BLOG" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YAO&#39;s BLOG</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">A personal blog of Yao Zhang</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" href="/projects">Project</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://psyyz10.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-SDA" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/11/SDA/" class="article-date">
  <time datetime="2015-11-09T19:36:06.000Z" itemprop="datePublished">2015-11-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Stacked Denoising Autoencoders
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Autoencoders">Autoencoders</h3><p>An autoencoder [4] is a network whose graphical structure is shown in Figure 4.1, which has the same dimension for both input and output. It takes an unlabeled training examples in set <img class="math" src="http://psyyz10.github.io/img/1.png"> where <img class="math" src="http://psyyz10.github.io/img/2.png"> is a single input and encodes it to the hidden layer <img class="math" src="http://psyyz10.github.io/img/3.png"> by linear combination with weight matrix <img class="math" src="http://psyyz10.github.io/img/4.png"> and then through a non-linear activation function. It can be mathematically expressed as <img class="math" src="http://psyyz10.github.io/img/5.png">, where <img class="math" src="http://psyyz10.github.io/img/6.png"> is the bias vector.</p>
<p><img src="http://psyyz10.github.io/img/7.png"><br>Figure 4.1: An Autoencoder</p>
<p>After that the hidden layer representation will be reconstructed to the output layer 􏰌x􏰌x􏰌x through a decoding function, in which 􏰌x􏰌x􏰌x has a same shape as x. Hence the decod- ing function can be mathematically expressed as 􏰌x􏰌x􏰌x = a(W′h+bh), where W′ can be W′ = WT called tried weights. In this project, tied weights were used. The aim of the model is to optimize the weight matrices, so that the reconstruction error be- tween input and output can be minimized. It can be seen that the Autoencoder can be viewed as an unsupervised learning process of encoding-decoding: the encoder encodes the input through multi-layer encoder and then the decoder will decode it back with the lowest error [30].<br>To measure the reconstruction error, traditional squared error L(x,􏰌x􏰌x􏰌x)=∥x−􏰌x􏰌x􏰌x∥ can be used. One of the most widely used way to measure that is the cross entropy if the input can be represented as bit vector or bit possibilities. The cross entropy error is shown in Equation 4.1:</p>
<p>The hidden layer code h can capture the information of input examples along the<br>main dimensions of variant coordinates via minimizing the reconstruction error. It is similar to the principle component analysis (PCA) which project data on the main component that captures the main information of the data. h can be viewed as a compression of input data with some lost, which hopefully not contain much information about the data. It is optimized to compress well the training data and have a small reconstruction error for the test data, but not for the data randomly chosen from input space.</p>
<h3 id="Denoising_Autoencoders">Denoising Autoencoders</h3><p>In order to prevent the Autoencoder from just learning the identity of the input and make the learnt representation more robust, it is better to reconstruct a corrupted version of the input. The Autoencoder with a corrupted version of input is called a Denoising Autoencoder. Its structure is shown in Figure 4.2. This method was proposed in [78], and it showed an advantage of corrupting the input by com- parative experiments. Hence we will use denoising autoencoders instead of classic autoencoders in this thesis.</p>
<p>Figure 4.2: A graphical figure of Denoising Autoencoder. An input x is corrupted to x ̃x ̃. After that the autoencoder maps it to the hidden representation h and attempts to reconstruct x.</p>
<p>A Denoising Autoencoder can be seen as a stochastic version with adding a stochas- tic corruption process to Autoencoder. For the raw inputs x, some of them will be randomly set to 0 as corrupted inputs x ̃x ̃. Next the corrupted input x ̃x ̃ will be en- coded to the hidden code and then reconstructed to the ouput. But now 􏰌x􏰌x􏰌x is a deterministic function of x ̃x ̃ rather than x. As Autoencoder, the reconstruction is considered and calculated between 􏰌x􏰌x􏰌x and x noted as L(x,􏰌x􏰌x􏰌x). The parameters of the model are initialized randomly and then optimized by stochastic gradient descent algorithms. The difference is that the input of the encoding process is a corrupted version x ̃x ̃, hence it forces a much more clever mapping than just the identity, which can denoise and extract useful features in a noise condition.<br>The training algorithm of a denoising autoencoder is summarized in Algorithm 2.</p>
<h3 id="Stacked_Autoencoder">Stacked Autoencoder</h3><p>Unsupervised pre-training<br>A Stacked Autoencoder is a multi-layer neural network which consists of Autoen- coders in each layer. Each layer’s input is from previous layer’s output. The greedy layer wise pre-training is an unsupervised approach that trains only one layer each time. Every layer is trained as a denoising autoencoder via minimising the cross entropy in reconstruction. Once the first ith layer has been trained, it can train the (i + 1)th layer by using the previous layer’s hidden representation as input. An example is shown below. Figure 4.3 shows the first step of a stacked autoencoder. It trains an autoencoder on raw input x to learn h1 by minimizing the reconstruction error L(x,􏰌x􏰌x􏰌x).</p>
<p>Figure 4.3: Step 1 in Stacked Autoencoders</p>
<p>Next step shown in Figure 4.4. The hidden representation h1 would be as ”raw in- 􏰌􏰌􏰌<br>put” to train another autoencoder by minimizing the reconstruction error L(h1, h1). Note that the error is calculated between previous latent feature representation h′ and the output h1. Parameters W2 and W2 will be optimized by the gradient descent algorithm. The new hidden representation h2 will be the ’raw input’ of the next layer.</p>
<p>The pre-training algorithm of stacked denoising autoencoder is summarized in al- gorithm 3.<br>Algorithm 3 Unsupervised preTrain SDA<br>Figure 4.4: Step 2 in Stacked Autoencoders</p>
<p>Supervised fine-tuning<br>At last once all the layers has been pre-trained, the next step called fine-tuning is performed. A supervised predictor will be extended to the last layer, such as support vector machine or a logistic regression layer. In this project, we chose a logistic regression layer. After that the network will be trained. A sample graph is shown in Figure 4.5. It can be seen that for each layer of the network, only the encoding hidden representation hi (i ∈ N) are considered. The fine-tuning step will train the whole network by back-propagation like training an Artificial Neural Network. A stacked denoising autoencoder is just replace each layer’s autoencoder with denoising autoencoder whilst keeping other things the same.<br>￼￼￼￼</p>
<style type="text/css">
img {
    border: 0;
    max-width: 100%;
}
img.math {
    vertical-align: middle;
    display: inline;
    height: 22px; 
}
</style>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://psyyz10.github.io/2015/11/SDA/" data-id="cigsfm0hw00004qu5eh7lzed1" class="article-share-link">Share</a>
      
        <a href="http://psyyz10.github.io/2015/11/SDA/#disqus_thread" class="article-comment-link">Comments</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/10/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hexo</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  
    
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/11/SDA/">Stacked Denoising Autoencoders</a>
          </li>
        
          <li>
            <a href="/2015/10/hello-world/">Hexo</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Yao Zhang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/projects" class="mobile-nav-link">Project</a>
  
</nav>
    
<script>
  var disqus_shortname = 'httppsyyz10githubio';
  
  var disqus_url = 'http://psyyz10.github.io/2015/11/SDA/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>