<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Stacked Denoising Autoencoders | YAO&#39;s BLOG</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="AutoencodersAn autoencoder [Bengio09] is a network whose graphical structure is shown in Figure 4.1, which has the same dimension for both input and output. It takes an unlabeled training examples in">
<meta property="og:type" content="article">
<meta property="og:title" content="Stacked Denoising Autoencoders">
<meta property="og:url" content="http://psyyz10.github.io/2015/11/SDA/index.html">
<meta property="og:site_name" content="YAO's BLOG">
<meta property="og:description" content="AutoencodersAn autoencoder [Bengio09] is a network whose graphical structure is shown in Figure 4.1, which has the same dimension for both input and output. It takes an unlabeled training examples in">
<meta property="og:image" content="http://psyyz10.github.io/img/1.png">
<meta property="og:image" content="http://psyyz10.github.io/img/2.png">
<meta property="og:image" content="http://psyyz10.github.io/img/3.png">
<meta property="og:image" content="http://psyyz10.github.io/img/4.png">
<meta property="og:image" content="http://psyyz10.github.io/img/5.png">
<meta property="og:image" content="http://psyyz10.github.io/img/6.png">
<meta property="og:image" content="http://psyyz10.github.io/img/7.png">
<meta property="og:image" content="http://psyyz10.github.io/img/8.png">
<meta property="og:image" content="http://psyyz10.github.io/img/8.png">
<meta property="og:image" content="http://psyyz10.github.io/img/9.png">
<meta property="og:image" content="http://psyyz10.github.io/img/10.png">
<meta property="og:image" content="http://psyyz10.github.io/img/11.png">
<meta property="og:image" content="http://psyyz10.github.io/img/36.png">
<meta property="og:image" content="http://psyyz10.github.io/img/12.png">
<meta property="og:image" content="http://psyyz10.github.io/img/13.png">
<meta property="og:image" content="http://psyyz10.github.io/img/14.png">
<meta property="og:image" content="http://psyyz10.github.io/img/15.png">
<meta property="og:image" content="http://psyyz10.github.io/img/16.png">
<meta property="og:image" content="http://psyyz10.github.io/img/14.png">
<meta property="og:image" content="http://psyyz10.github.io/img/9.png">
<meta property="og:image" content="http://psyyz10.github.io/img/8.png">
<meta property="og:image" content="http://psyyz10.github.io/img/16.png">
<meta property="og:image" content="http://psyyz10.github.io/img/16.png">
<meta property="og:image" content="http://psyyz10.github.io/img/8.png">
<meta property="og:image" content="http://psyyz10.github.io/img/16.png">
<meta property="og:image" content="http://psyyz10.github.io/img/9.png">
<meta property="og:image" content="http://psyyz10.github.io/img/16.png">
<meta property="og:image" content="http://psyyz10.github.io/img/9.png">
<meta property="og:image" content="http://psyyz10.github.io/img/18.png">
<meta property="og:image" content="http://psyyz10.github.io/img/16.png">
<meta property="og:image" content="http://psyyz10.github.io/img/19.png">
<meta property="og:image" content="http://psyyz10.github.io/img/20.png">
<meta property="og:image" content="http://psyyz10.github.io/img/21.png">
<meta property="og:image" content="http://psyyz10.github.io/img/9.png">
<meta property="og:image" content="http://psyyz10.github.io/img/22.png">
<meta property="og:image" content="http://psyyz10.github.io/img/18.png">
<meta property="og:image" content="http://psyyz10.github.io/img/34.png">
<meta property="og:image" content="http://psyyz10.github.io/img/22.png">
<meta property="og:image" content="http://psyyz10.github.io/img/25.png">
<meta property="og:image" content="http://psyyz10.github.io/img/24.png">
<meta property="og:image" content="http://psyyz10.github.io/img/26.png">
<meta property="og:image" content="http://psyyz10.github.io/img/27.png">
<meta property="og:image" content="http://psyyz10.github.io/img/28.png">
<meta property="og:image" content="http://psyyz10.github.io/img/29.png">
<meta property="og:image" content="http://psyyz10.github.io/img/30.png">
<meta property="og:image" content="http://psyyz10.github.io/img/31.png">
<meta property="og:image" content="http://psyyz10.github.io/img/32.png">
<meta property="og:image" content="http://psyyz10.github.io/img/33.png">
<meta property="og:image" content="http://psyyz10.github.io/img/35.png">
<meta property="og:updated_time" content="2015-11-10T03:32:25.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stacked Denoising Autoencoders">
<meta name="twitter:description" content="AutoencodersAn autoencoder [Bengio09] is a network whose graphical structure is shown in Figure 4.1, which has the same dimension for both input and output. It takes an unlabeled training examples in">
  
    <link rel="alternative" href="/atom.xml" title="YAO&#39;s BLOG" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YAO&#39;s BLOG</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">A personal blog of Yao Zhang</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" href="/projects">Project</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://psyyz10.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-SDA" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/11/SDA/" class="article-date">
  <time datetime="2015-11-09T19:36:06.000Z" itemprop="datePublished">2015-11-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Stacked Denoising Autoencoders
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Autoencoders">Autoencoders</h2><p>An autoencoder <a href="#Bengio09">[Bengio09]</a> is a network whose graphical structure is shown in Figure <a href="F4.1">4.1</a>, which has the same dimension for both input and output. It takes an unlabeled training examples in set <img class="math" src="http://psyyz10.github.io/img/1.png"> where <img class="math" src="http://psyyz10.github.io/img/2.png"> is a single input and encodes it to the hidden layer <img class="math" src="http://psyyz10.github.io/img/3.png"> by linear combination with weight matrix <img class="math" style="height:18px" src="http://psyyz10.github.io/img/4.png"> and then through a non-linear activation function. It can be mathematically expressed as <img class="math" src="http://psyyz10.github.io/img/5.png">, where <img class="math" style="height:18px" src="http://psyyz10.github.io/img/6.png"> is the bias vector.</p>
<center><br>    <img style="height:300px" src="http://psyyz10.github.io/img/7.png"><br>    <a name="F4.1"><br>        <p>Figure 4.1: An Autoencoder</p><br>    </a><br></center>

<p>After that the hidden layer representation will be reconstructed to the output layer 􏰌<img class="math" src="http://psyyz10.github.io/img/8.png"> through a decoding function, in which 􏰌<img class="math" src="http://psyyz10.github.io/img/8.png"> has a same shape as <img class="math" src="http://psyyz10.github.io/img/9.png">. Hence the decoding function can be mathematically expressed as 􏰌<img class="math" src="http://psyyz10.github.io/img/10.png">, where <img class="math" src="http://psyyz10.github.io/img/11.png"> can be <img class="math" src="http://psyyz10.github.io/img/36.png"> called tried weights. In this project, tied weights were used. The aim of the model is to optimize the weight matrices, so that the reconstruction error between input and output can be minimized. It can be seen that the Autoencoder can be viewed as an unsupervised learning process of encoding-decoding: the encoder encodes the input through multi-layer encoder and then the decoder will decode it back with the lowest error <a href="Hinton06"></a>.<br>To measure the reconstruction error, traditional squared error <img class="math" src="http://psyyz10.github.io/img/12.png"> can be used. One of the most widely used way to measure that is the cross entropy if the input can be represented as bit vector or bit possibilities. The cross entropy error is shown in Equation <a href="E4.1">4.1</a>:</p>
<p></p><p><br>    <a name="E4.1"><img style="height:60px; display: inline;" src="http://psyyz10.github.io/img/13.png"></a><br></p><br>The hidden layer code <img class="math" src="http://psyyz10.github.io/img/14.png"> can capture the information of input examples along the main dimensions of variant coordinates via minimizing the reconstruction error. It is similar to the principle component analysis (PCA) which project data on the main component that captures the main information of the data. h can be viewed as a compression of input data with some lost, which hopefully not contain much information about the data. It is optimized to compress well the training data and have a small reconstruction error for the test data, but not for the data randomly chosen from input space.<p></p>
<h2 id="Denoising_Autoencoders">Denoising Autoencoders</h2><p>In order to prevent the Autoencoder from just learning the identity of the input and make the learnt representation more robust, it is better to reconstruct a corrupted version of the input. The Autoencoder with a corrupted version of input is called a Denoising Autoencoder. Its structure is shown in Figure <a href="F4.2">4.2</a>. This method was proposed in <a href="Vincent08">[78]</a>, and it showed an advantage of corrupting the input by comparative experiments. Hence we will use denoising autoencoders instead of classic autoencoders in this thesis.<br><a name="F4.2"><br>    <center><br>        <img style="height:200px" src="http://psyyz10.github.io/img/15.png"><br>        <p>Figure 4.2: A graphical figure of Denoising Autoencoder. An input x is corrupted to <img class="math" src="http://psyyz10.github.io/img/16.png">. After that the autoencoder maps it to the hidden representation <img class="math" src="http://psyyz10.github.io/img/14.png"> and attempts to reconstruct <img class="math" src="http://psyyz10.github.io/img/9.png">.</p><br>    </center><br></a></p>
<p>A Denoising Autoencoder can be seen as a stochastic version with adding a stochastic corruption process to Autoencoder. For the raw inputs <img class="math" src="http://psyyz10.github.io/img/8.png">, some of them will be randomly set to 0 as corrupted inputs <img class="math" src="http://psyyz10.github.io/img/16.png">. Next the corrupted input <img class="math" src="http://psyyz10.github.io/img/16.png"> will be en- coded to the hidden code and then reconstructed to the ouput. But now 􏰌<img class="math" src="http://psyyz10.github.io/img/8.png"> is a deterministic function of <img class="math" src="http://psyyz10.github.io/img/16.png"> rather than <img class="math" src="http://psyyz10.github.io/img/9.png">. As Autoencoder, the reconstruction is considered and calculated between 􏰌<img class="math" src="http://psyyz10.github.io/img/16.png"> and <img class="math" src="http://psyyz10.github.io/img/9.png"> noted as <img class="math" src="http://psyyz10.github.io/img/18.png">. The parameters of the model are initialized randomly and then optimized by stochastic gradient descent algorithms. The difference is that the input of the encoding process is a corrupted version <img class="math" src="http://psyyz10.github.io/img/16.png">, hence it forces a much more clever mapping than just the identity, which can denoise and extract useful features in a noise condition.<br>The training algorithm of a denoising autoencoder is summarized in Algorithm 2.</p>
<center><br>    <img style="width:70%" src="http://psyyz10.github.io/img/19.png"><br></center>

<h2 id="Stacked_Autoencoder">Stacked Autoencoder</h2><p>Unsupervised pre-training<br>A Stacked Autoencoder is a multi-layer neural network which consists of Autoencoders in each layer. Each layer’s input is from previous layer’s output. The greedy layer wise pre-training is an unsupervised approach that trains only one layer each time. Every layer is trained as a denoising autoencoder via minimising the cross entropy in reconstruction. Once the first <img class="math" src="http://psyyz10.github.io/img/20.png"> layer has been trained, it can train the <img class="math" src="http://psyyz10.github.io/img/21.png"> layer by using the previous layer’s hidden representation as input. An example is shown below. Figure <a href="E4.3">4.3</a> shows the first step of a stacked autoencoder. It trains an autoencoder on raw input <img class="math" src="http://psyyz10.github.io/img/9.png"> to learn <img class="math" src="http://psyyz10.github.io/img/22.png"> by minimizing the reconstruction error <img class="math" src="http://psyyz10.github.io/img/18.png">.<br><a href="E4.3"><br>    <center><br>        <img style="height:150px" src="http://psyyz10.github.io/img/34.png"><br>        <p>Figure 4.3: Step 1 in Stacked Autoencoders</p><br>    </center><br></a></p>
<p>Next step shown in Figure <a name="E4.4">4.4</a>. The hidden representation <img class="math" src="http://psyyz10.github.io/img/22.png"> would be as ”raw input” to train another autoencoder by minimizing the reconstruction error <img class="math" src="http://psyyz10.github.io/img/25.png">. Note that the error is calculated between previous latent feature representation <img class="math" src="http://psyyz10.github.io/img/24.png"> and the output <img class="math" src="http://psyyz10.github.io/img/26.png">. Parameters <img class="math" src="http://psyyz10.github.io/img/27.png"> and <img class="math" src="http://psyyz10.github.io/img/28.png"> will be optimized by the gradient descent algorithm. The new hidden representation h2 will be the ’raw input’ of the next layer.<br><a href=""><br>    <center><br>        <img style="height:225px" src="http://psyyz10.github.io/img/29.png"><br>        <p>Figure 4.4: Step 2 in Stacked Autoencoders</p><br>    </center><br></a><br>The pre-training algorithm of stacked denoising autoencoder is summarized in algorithm 3.</p>
<center><br>    <img style="width:70%" src="http://psyyz10.github.io/img/30.png"><br></center>



<h2 id="Supervised_fine-tuning">Supervised fine-tuning</h2><p>At last once all the layers has been pre-trained, the next step called fine-tuning is performed. A supervised predictor will be extended to the last layer, such as support vector machine or a logistic regression layer. In this project, we chose a logistic regression layer. After that the network will be trained. A sample graph is shown in Figure <a name="F4.5">4.5</a>. It can be seen that for each layer of the network, only the encoding hidden representation <img class="math" src="http://psyyz10.github.io/img/31.png"> are considered. The fine-tuning step will train the whole network by back-propagation like training an Artificial Neural Network. A stacked denoising autoencoder is just replace each layer’s autoencoder with denoising autoencoder whilst keeping other things the same.<br><a href=""><br>    <center><br>        <img style="height:300px" src="http://psyyz10.github.io/img/32.png"><br>        <p>Figure 4.5: A complete architecture of stacked autoencoder</p><br>    </center><br></a></p>
<p>The supervised fine-tuning algorithm of stacked denoising auto-encoder is summa- rized in Algorithm 4.</p>
<center><br>    <img style="width:70%" src="http://psyyz10.github.io/img/33.png"><br>    <img style="width:70%" src="http://psyyz10.github.io/img/35.png"><br></center>

<p><a name="Bengio09"><br>    <p>Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127, 2009. Also published as a book. Now Publishers, 2009.</p><br></a><br><a name="Hinton06"><br>    <p>Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.</p><br></a><br><a href="Vincent08"><br>    <p>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Man- zagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM, 2008.</p><br></a></p>
<p>￼￼￼￼</p>
<style type="text/css">
img {
    border: 0;
    max-width: 100%;
}
img.math {
    vertical-align: middle;
    display: inline;
    height: 22px; 
}
</style>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://psyyz10.github.io/2015/11/SDA/" data-id="cigsto5pp0006xvu58oyqvsgi" class="article-share-link">Share</a>
      
        <a href="http://psyyz10.github.io/2015/11/SDA/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2015/11/Deep-Learning-Overview/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Deep Learning Overview
        
      </div>
    </a>
  
  
    <a href="/2015/10/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hexo</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a><span class="tag-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/11/Deep-Learning-Overview/">Deep Learning Overview</a>
          </li>
        
          <li>
            <a href="/2015/11/SDA/">Stacked Denoising Autoencoders</a>
          </li>
        
          <li>
            <a href="/2015/10/hello-world/">Hexo</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Yao Zhang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/projects" class="mobile-nav-link">Project</a>
  
</nav>
    
<script>
  var disqus_shortname = 'httppsyyz10githubio';
  
  var disqus_url = 'http://psyyz10.github.io/2015/11/SDA/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>