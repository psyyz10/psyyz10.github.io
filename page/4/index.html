<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>YAO&#39;s BLOG</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="YAO's BLOG">
<meta property="og:url" content="http://psyyz10.github.io/page/4/index.html">
<meta property="og:site_name" content="YAO's BLOG">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="YAO's BLOG">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="YAO&#39;s BLOG" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">YAO&#39;s BLOG</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">A personal blog of Yao Zhang</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" href="/projects">Project</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://psyyz10.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Remove-Duplicates-from-Sorted-Array" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/11/Remove-Duplicates-from-Sorted-Array/" class="article-date">
  <time datetime="2015-11-10T08:52:47.000Z" itemprop="datePublished">2015-11-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/Remove-Duplicates-from-Sorted-Array/">Remove Duplicates from Sorted Array</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Problem:">Problem:</h2><p>Given a sorted array, remove the duplicates in place such that each element appear only once and return the new length.</p>
<p>Do not allocate extra space for another array, you must do this in place with constant memory.</p>
<p>Example<br>Given input array A = [1,1,2],</p>
<p>Your function should return length = 2, and A is now [1,2].</p>
<p>Tags Expand  </p>
<p>Related Problems Expand </p>
<p><a href="https://leetcode.com/problems/remove-duplicates-from-sorted-array/" target="_blank" rel="external">Leetcode link</a><br><a href="http://www.lintcode.com/en/problem/remove-duplicates-from-sorted-array/" target="_blank" rel="external">lintcode link</a></p>
<h2 id="My_Solution:">My Solution:</h2><p>In this question, a variable named ‘last’ is used to record the index of last element in the result array.<br>Go through the array, if the current element nums[i] is not equal to the  last element of result array ‘nums[last]’,<br>add the current element to end of result array.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*************************************************************************</span><br><span class="line">    &gt; File Name: RemoveDuplicate2.java</span><br><span class="line">    &gt; Author: Yao Zhang                                                                                                                                                              </span><br><span class="line">    &gt; Mail: psyyz10@163.com</span><br><span class="line">    &gt; Created Time: Thu 29 Oct 15:26:17 2015</span><br><span class="line"> ************************************************************************/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RemoveDuplicate</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">removeDuplicates</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums == <span class="keyword">null</span> || nums.length == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">int</span> last = <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.length; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[last] != nums[i])&#123;</span><br><span class="line">                nums[++last] = nums[i];</span><br><span class="line">            &#125;   </span><br><span class="line">        &#125;   </span><br><span class="line">              </span><br><span class="line">        <span class="keyword">return</span> ++last;</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Time complexity is O(n), space complexity is O(1)</p>
<h5 id="Related_Quetion:">Related Quetion:</h5><p><a href="http://psyyz10.github.io/2015/11/Remove-Duplicates-from-Sorted-Array-II/">Remove Duplicates from Sorted Array II</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://psyyz10.github.io/2015/11/Remove-Duplicates-from-Sorted-Array/" data-id="cih2zfgus0011nju5uexevuy2" class="article-share-link">Share</a>
      
        <a href="http://psyyz10.github.io/2015/11/Remove-Duplicates-from-Sorted-Array/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Leetcode-and-Lintcode/">Leetcode and Lintcode</a><span class="article-tag-list-count">29</span></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Deep-Learning-Overview" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/11/Deep-Learning-Overview/" class="article-date">
  <time datetime="2015-11-10T01:01:13.000Z" itemprop="datePublished">2015-11-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/Deep-Learning-Overview/">Deep Learning Overview</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Nowadays, the performances of machine learning models heavily rely on the representation of data or feature selection steps rather than just the choice of machine learning algorithms. Thus much effort is applied on preprocessing pipelines such as feature selection. Even though some specific domain knowledge can be used to help design the representation of data, the motivation of Artificial Intelligence needs more powerful representations of features. Deep Learning also called unsupervised learning, is a relatively new research field of machine learning which can learn multi- ple levels of abstraction and representation of features directly from data. It aims at learning feature hierarchies with higher level features formed by the composition of lower ones. The multiple levels structures allow to build complex functions which take data as input and output the result directly without depending on features crafted by humans <a href="#Bengio09">[Bengio09]</a>.</p>
<p>Deep learning achieved many successful results on some problems, such as im- age classification <a href="#Ciresan12">[Ciresan12]</a> <a href="#Krizhevsky12">[Krizhevsky12]</a>, semantic parsing <a href="#Bordes12">[Bordes12]</a> and speech recognition <a href="#Dahl12">[Dahl12]</a>. Deep architecture may express the complex distributions more efficiently with better per- formance on challenging tasks <a href="#Bengio07">[Bengio07]</a><a href="#Bengio09">[Bengio09]</a>. The hypothesis that the composition of addi- tional functional levels can give more powerful modeling capacity has already been proposed for a long time <a href="#Hinton90">[Hinton90]</a><a href="#Rumelhart86">[Rumelhart86]</a>. However, the training process of deep architec- ture was proven to be very difficult, until some successful approaches of <a href="#Bengio007">[Bengio007]</a><a href="#Hinton06">[Hinton06]</a><a href="#Hinton006">[Hinton006]</a> for training stacked autoencoder and DBN occurred. One key idea behind them is to train the deep architecture layer by layer by unsupervised learning, which is also called unsupervised feature learning. Each layer will generate a more abstract representation of the observed layer by doing a local optimization. Unsupervised feature learning can learn useful features automatically and directly from the data set by unsupervised learning without given specific features defined by human. The unsupervised learned features are more natural with less information lost <a href="#Bengio15">[Bengio15]</a>. Some deep learning models also have a potential powerful capacity of solving time-series problems <a href="#La14">[La14]</a>, which is another reason that makes deep learning suitable for stock trend prediction. Therefore deep learning can provide a new potential and powerful approach to improve stock prediction.</p>
<h2 id="Building_Deep_Representations">Building Deep Representations</h2><p>Experimental results show that it is much harder to train a deep architecture than training a shallow one <a href="#Bengio07">[Bengio07]</a><a href="#Erhan09">[Erhan09]</a>. The rapid recent growth and success of deep learning owe to a breakthrough initiated by Geoff Hinton in 2006 and quickly followed up by some papers <a href="#Hinton006">[Hinton006]</a><a href="#Bengio94">[Bengio94]</a><a href="#Poultney06">[Poultney06]</a>. Greedy layer wise unsupervised pre-training as a central idea was proposed. It means just one layer of the hierarchy is trained at one time by unsupervised feature learning to learn a new transformation of data with the previous layer output. Finally, the set of pre-trained layers is combined with a stan- dard supervised classifier such as Support Vector Machine, Logistic Regression or as initialization for a deep learning model such as a Stacked Denoising Auto-encoder or Deep Belief Network. Experiments show that the layer-wise stacking can attain a better feature representation in most time <a href="#Larochelle09">[Larochelle09]</a>. Although it is not difficult to combine single layers pretrained by unsupervised learning into a supervised model, it is not very clear how the single layers should combine to form a better unsuper- vised model <a href="#Bengio15">[Bengio15]</a>. One approach is to stack pre-trained RBMs (section 4.2.1) into DBN (section 4.2.2).</p>
<p><a href="http://psyyz10.github.io/2015/11/SDA/"><h3>Stacked Denoising Autoencoders</h3></a></p>
<p><br><a name="Bengio09">[Bengio09]</a> Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127, 2009. Also published as a book. Now Publishers, 2009.<br><br><a name="Ciresan12">[Ciresan12]</a> Dan Ciresan, Ueli Meier, and Ju ̈rgen Schmidhuber. Multi-column deep neural networks for image classification. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3642–3649. IEEE, 2012.<br><br><a name="Krizhevsky12">[Krizhevsky12]</a> Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.<br><br><a name="Bordes12">[Bordes12]</a> Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. Joint learn- ing of words and meaning representations for open-text semantic parsing. In International Conference on Artificial Intelligence and Statistics, pages 127– 135, 2012.<br><br><a name="Dahl12">[Dahl12]</a> George E Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre- trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42, 2012.<br><br><a name="Bengio07">[Bengio07]</a> Yoshua Bengio, Yann LeCun, et al. Scaling learning algorithms towards ai. Large-scale kernel machines, 34(5), 2007.<br><br><a name="Hinton90">[Hinton90]</a> Geoffrey E Hinton. Connectionist learning procedures. artificial intelligence, 40 1-3: 185 234, 1989. reprinted in j. carbonell, editor,”. Machine Learning: Paradigms and Methods”, MIT Press, 1990.<br><br><a name="Rumelhart86">[Rumelhart86]</a> David E Rumelhart and James L McClelland. The pdp research group: Par- allel distributed processing: Explorations in the microstructure of cognition. Foundations, 1, 1986.<br><br><a name="Bengio007">[Bengio007]</a> Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. Greedy layer-wise training of deep networks. Advances in neural information processing systems, 19:153, 2007.<br><br><a name="Hinton06">[Hinton06]</a> Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.<br><br><a name="Hinton006">[Hinton006]</a> Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algo- rithm for deep belief nets. Neural computation, 18(7):1527–1554, 2006.<br><br><a name="Erhan09">[Erhan09]</a> Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, and Pascal Vincent. The difficulty of training deep architectures and the effect of unsupervised pre-training. In International Conference on artificial intelligence and statistics, pages 153–160, 2009.<br><br><a name="Bengio15">[Bengio15]</a> Yoshua Bengio, Ian J. Goodfellow, and Aaron Courville. Deep learning. Book in preparation for MIT Press, 2015.<br><br><a name="La14">[La14]</a> Martin La ̈ngkvist, Lars Karlsson, and Amy Loutfi. A review of unsupervised feature learning and deep learning for time-series modeling. Pattern Recogni- tion Letters, 42:11–24, 2014.<br><br><a name="Bengio94">[Bengio94]</a> Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term depen- dencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.<br><br><a name="Poultney06">[Poultney06]</a> Christopher Poultney, Sumit Chopra, Yann L Cun, et al. Efficient learning of sparse representations with an energy-based model. In Advances in neural information processing systems, pages 1137–1144, 2006.<br><br><a name="Larochelle09">[Larochelle09]</a> Hugo Larochelle, Yoshua Bengio, J ́eroˆme Louradour, and Pascal Lamblin. Ex- ploring strategies for training deep neural networks. The Journal of Machine Learning Research, 10:1–40, 2009.<br><br><a name="Bengio15">[Bengio15]</a> Yoshua Bengio, Ian J. Goodfellow, and Aaron Courville. Deep learning. Book in preparation for MIT Press, 2015.<br></p>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://psyyz10.github.io/2015/11/Deep-Learning-Overview/" data-id="cih2zfgvm001lnju5v7k2r3mh" class="article-share-link">Share</a>
      
        <a href="http://psyyz10.github.io/2015/11/Deep-Learning-Overview/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a><span class="article-tag-list-count">2</span></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-SDA" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/11/SDA/" class="article-date">
  <time datetime="2015-11-09T19:36:06.000Z" itemprop="datePublished">2015-11-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/SDA/">Stacked Denoising Autoencoders</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Autoencoders">Autoencoders</h2><p>An autoencoder <a href="#Bengio09">[Bengio09]</a> is a network whose graphical structure is shown in Figure <a href="#F4_1">4.1</a>, which has the same dimension for both input and output. It takes an unlabeled training examples in set <img class="math" src="http://psyyz10.github.io/img/1.png"> where <img class="math" src="http://psyyz10.github.io/img/2.png"> is a single input and encodes it to the hidden layer <img class="math" src="http://psyyz10.github.io/img/3.png"> by linear combination with weight matrix <img class="math" style="height:18px" src="http://psyyz10.github.io/img/4.png"> and then through a non-linear activation function. It can be mathematically expressed as <img class="math" src="http://psyyz10.github.io/img/5.png">, where <img class="math" style="height:18px" src="http://psyyz10.github.io/img/6.png"> is the bias vector.</p>
<center><br>    <img id="F4_1" style="height:300px" src="http://psyyz10.github.io/img/7.png"><br>    <p>Figure 4.1: An Autoencoder</p><br></center>

<p>After that the hidden layer representation will be reconstructed to the output layer 􏰌<img class="math" src="http://psyyz10.github.io/img/8.png"> through a decoding function, in which 􏰌<img class="math" src="http://psyyz10.github.io/img/8.png"> has a same shape as <img class="math" src="http://psyyz10.github.io/img/9.png">. Hence the decoding function can be mathematically expressed as 􏰌<img class="math" src="http://psyyz10.github.io/img/10.png">, where <img class="math" src="http://psyyz10.github.io/img/11.png"> can be <img class="math" src="http://psyyz10.github.io/img/36.png"> called tried weights. In this project, tied weights were used. The aim of the model is to optimize the weight matrices, so that the reconstruction error between input and output can be minimized. It can be seen that the Autoencoder can be viewed as an unsupervised learning process of encoding-decoding: the encoder encodes the input through multi-layer encoder and then the decoder will decode it back with the lowest error <a href="#Hinton06">[Hinton06]</a>.<br>To measure the reconstruction error, traditional squared error <img class="math" src="http://psyyz10.github.io/img/12.png"> can be used. One of the most widely used way to measure that is the cross entropy if the input can be represented as bit vector or bit possibilities. The cross entropy error is shown in Equation <a href="#E4_1">4.1</a>:</p>
<center><br>    <img id="E4_1" style="height:60px" src="http://psyyz10.github.io/img/13.png"><br>    <p>Equation:4.1</p><br></center>

<p>The hidden layer code <img class="math" src="http://psyyz10.github.io/img/14.png"> can capture the information of input examples along the main dimensions of variant coordinates via minimizing the reconstruction error. It is similar to the principle component analysis (PCA) which project data on the main component that captures the main information of the data. h can be viewed as a compression of input data with some lost, which hopefully not contain much information about the data. It is optimized to compress well the training data and have a small reconstruction error for the test data, but not for the data randomly chosen from input space.</p>
<h2 id="Denoising_Autoencoders">Denoising Autoencoders</h2><p>In order to prevent the Autoencoder from just learning the identity of the input and make the learnt representation more robust, it is better to reconstruct a corrupted version of the input. The Autoencoder with a corrupted version of input is called a Denoising Autoencoder. Its structure is shown in Figure <a href="#F4_2">4.2</a>. This method was proposed in <a href="#Vincent08">[Vincent08]</a>, and it showed an advantage of corrupting the input by comparative experiments. Hence we will use denoising autoencoders instead of classic autoencoders in this thesis.</p>
<center><br>    <img id="F4_2" style="height:200px" src="http://psyyz10.github.io/img/15.png"><br>    <p>Figure 4.2: A graphical figure of Denoising Autoencoder. An input x is corrupted to <img class="math" src="http://psyyz10.github.io/img/16.png">. After that the autoencoder maps it to the hidden representation <img class="math" src="http://psyyz10.github.io/img/14.png"> and attempts to reconstruct <img class="math" src="http://psyyz10.github.io/img/9.png">.</p><br></center>

<p>A Denoising Autoencoder can be seen as a stochastic version with adding a stochastic corruption process to Autoencoder. For the raw inputs <img class="math" src="http://psyyz10.github.io/img/8.png">, some of them will be randomly set to 0 as corrupted inputs <img class="math" src="http://psyyz10.github.io/img/16.png">. Next the corrupted input <img class="math" src="http://psyyz10.github.io/img/16.png"> will be en- coded to the hidden code and then reconstructed to the ouput. But now 􏰌<img class="math" src="http://psyyz10.github.io/img/8.png"> is a deterministic function of <img class="math" src="http://psyyz10.github.io/img/16.png"> rather than <img class="math" src="http://psyyz10.github.io/img/9.png">. As Autoencoder, the reconstruction is considered and calculated between 􏰌<img class="math" src="http://psyyz10.github.io/img/16.png"> and <img class="math" src="http://psyyz10.github.io/img/9.png"> noted as <img class="math" src="http://psyyz10.github.io/img/18.png">. The parameters of the model are initialized randomly and then optimized by stochastic gradient descent algorithms. The difference is that the input of the encoding process is a corrupted version <img class="math" src="http://psyyz10.github.io/img/16.png">, hence it forces a much more clever mapping than just the identity, which can denoise and extract useful features in a noise condition.</p>
<p>The training algorithm of a denoising autoencoder is summarized in Algorithm 2.</p>
<center><br>    <img style="width:70%" src="http://psyyz10.github.io/img/19.png"><br></center>

<h2 id="Stacked_Autoencoder">Stacked Autoencoder</h2><p>Unsupervised pre-training<br>A Stacked Autoencoder is a multi-layer neural network which consists of Autoencoders in each layer. Each layer’s input is from previous layer’s output. The greedy layer wise pre-training is an unsupervised approach that trains only one layer each time. Every layer is trained as a denoising autoencoder via minimising the cross entropy in reconstruction. Once the first <img class="math" src="http://psyyz10.github.io/img/20.png"> layer has been trained, it can train the <img class="math" src="http://psyyz10.github.io/img/21.png"> layer by using the previous layer’s hidden representation as input. An example is shown below. Figure <a href="#E4_3">4.3</a> shows the first step of a stacked autoencoder. It trains an autoencoder on raw input <img class="math" src="http://psyyz10.github.io/img/9.png"> to learn <img class="math" src="http://psyyz10.github.io/img/22.png"> by minimizing the reconstruction error <img class="math" src="http://psyyz10.github.io/img/18.png">.</p>
<center><br>    <img id="E4_3" style="height:150px" src="http://psyyz10.github.io/img/34.png"><br>    <p>Figure 4.3: Step 1 in Stacked Autoencoders</p><br></center>

<p>Next step shown in Figure <a href="#F4_4">4.4</a>. The hidden representation <img class="math" src="http://psyyz10.github.io/img/22.png"> would be as ”raw input” to train another autoencoder by minimizing the reconstruction error <img class="math" src="http://psyyz10.github.io/img/25.png">. Note that the error is calculated between previous latent feature representation <img class="math" src="http://psyyz10.github.io/img/24.png"> and the output <img class="math" src="http://psyyz10.github.io/img/26.png">. Parameters <img class="math" src="http://psyyz10.github.io/img/27.png"> and <img class="math" src="http://psyyz10.github.io/img/28.png"> will be optimized by the gradient descent algorithm. The new hidden representation h2 will be the ’raw input’ of the next layer.</p>
<center><br>    <img id="F4_4" style="height:225px" src="http://psyyz10.github.io/img/29.png"><br>    <p>Figure 4.4: Step 2 in Stacked Autoencoders </p><br></center><br>The pre-training algorithm of stacked denoising autoencoder is summarized in algorithm 3.<br><center><br>    <img style="width:70%" src="http://psyyz10.github.io/img/30.png"><br></center>

<h2 id="Supervised_fine-tuning">Supervised fine-tuning</h2><p>At last once all the layers has been pre-trained, the next step called fine-tuning is performed. A supervised predictor will be extended to the last layer, such as support vector machine or a logistic regression layer. In this project, we chose a logistic regression layer. After that the network will be trained. A sample graph is shown in Figure <a href="#F4_5">4.5</a>. It can be seen that for each layer of the network, only the encoding hidden representation <img class="math" src="http://psyyz10.github.io/img/31.png"> are considered. The fine-tuning step will train the whole network by back-propagation like training an Artificial Neural Network. A stacked denoising autoencoder is just replace each layer’s autoencoder with denoising autoencoder whilst keeping other things the same.</p>
<center><br>    <img id="F4_5" style="height:300px" src="http://psyyz10.github.io/img/32.png"><br>    <p>Figure 4.5: A complete architecture of stacked autoencoder</p><br></center>

<p>The supervised fine-tuning algorithm of stacked denoising auto-encoder is summa- rized in Algorithm 4.</p>
<center><br>    <img style="width:70%" src="http://psyyz10.github.io/img/33.png"><br>    <img style="width:70%" src="http://psyyz10.github.io/img/35.png"><br></center>

<p></p><p><br><a name="Bengio09">[Bengio09]</a> Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127, 2009. Also published as a book. Now Publishers, 2009.</p>
<p><a name="Hinton06">[Hinton06]</a> Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006</p>
<p><a name="Vincent08">[Vincent08]</a> Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Man- zagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM, 2008.<br></p><br>￼￼￼￼<p></p>
<style type="text/css">
img {
    border: 0;
    max-width: 100%;
}
img.math {
    vertical-align: middle;
    display: inline;
    height: 22px; 
}
</style>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://psyyz10.github.io/2015/11/SDA/" data-id="cih2zfgtv000qnju5luxktfnp" class="article-share-link">Share</a>
      
        <a href="http://psyyz10.github.io/2015/11/SDA/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a><span class="article-tag-list-count">2</span></li></ul>

    </footer>
  </div>
  
</article>


  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span>
    </nav>
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Leetcode-and-Lintcode/">Leetcode and Lintcode</a><span class="tag-list-count">29</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Leetcode-and-Lintcode/" style="font-size: 20px;">Leetcode and Lintcode</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">33</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/11/Remove-Duplicates-from-Sorted-List/">Remove Duplicates from Sorted List</a>
          </li>
        
          <li>
            <a href="/2015/11/Partition-List/">Partition List</a>
          </li>
        
          <li>
            <a href="/2015/11/Reverse-Linked-List-II/">Reverse Linked List II</a>
          </li>
        
          <li>
            <a href="/2015/11/Add-Two-Numbers-II/">Add Two Numbers II</a>
          </li>
        
          <li>
            <a href="/2015/11/Add-Two-Numbers/">Add Two Numbers</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Yao Zhang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/projects" class="mobile-nav-link">Project</a>
  
</nav>
    
<script>
  var disqus_shortname = 'httppsyyz10githubio';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>